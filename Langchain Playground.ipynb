{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Enviroment Testing\n",
    "\n",
    "- This is the code that will be packaged into the lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. convert dataset\n",
    "# df = pd.read_parquet('ml-arxiv-papers/data/train-00000-of-00001-52427cf3bce60f12.parquet')\n",
    "# df.head(1000).to_csv('ml-arxiv-papers/csv_data/train.csv', index=False)\n",
    "# df.head(50000).to_csv('ml-arxiv-papers/csv_data/train2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. vectorize paper data\n",
    "# os.getenv(\"OPEN_AI_KEY\")\n",
    "\n",
    "# loader = CSVLoader(file_path='ml-arxiv-papers/csv_data/train2.csv')\n",
    "# documents = loader.load()\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# db = FAISS.from_documents(documents, embeddings)\n",
    "# db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. function for similiarity search \n",
    "\n",
    "def retrieve_info(query):\n",
    "    similiar_response = db.similarity_search(query, k=3)\n",
    "    page_contents_array = [doc.page_content for doc in similiar_response]\n",
    "    return page_contents_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Setup LLm Chain and prompt \n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "template = \"\"\" \n",
    "You are a world class research paper writer.\n",
    "I will share a title and context for a research paper with you and you will \\\n",
    "give the abstract that goes along with that paper based on past abstracts, \\\n",
    "and you will follow all the rules below:\n",
    "\n",
    "1. The response should be very similiar or identical to best practices in terms of ton of voice, sentence structure and other concepts.\n",
    "\n",
    "2. If the abstracts are irrelevent then try to mimic the style of the abstracts.\n",
    "\n",
    "3. Make each response approximately 200 words long\n",
    "\n",
    "Below is the title of the paper:\n",
    "{title}\n",
    "\n",
    "Here is some context:\n",
    "{context}\n",
    "\n",
    "Here is a list of abstracts of similiar papers:\n",
    "{abstracts}\n",
    "\n",
    "Please write the best abstract for this paper:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"title\",\"context\",\"abstracts\"],\n",
    "    template=template\n",
    "    )\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches: \n",
      "['title: Learning to Catch Piglets in Flight\\nabstract: Catching objects in-flight is an outstanding challenge in robotics. In this paper, we present a closed-loop control system fusing data from two sensor modalities: an RGB-D camera and a radar. To develop and test our method, we start with an easy to identify object: a stuffed Piglet. We implement and compare two approaches to detect and track the object, and to predict the interception point. A baseline model uses colour filtering for locating the thrown object in the environment, while the interception point is predicted using a least squares regression over the physical ballistic trajectory equations. A deep learning based method uses artificial neural networks for both object detection and interception point prediction. We show that we are able to successfully catch Piglet in 80% of the cases with our deep learning approach.', 'title: Learned Visual Navigation for Under-Canopy Agricultural Robots\\nabstract: We describe a system for visually guided autonomous navigation of under-canopy farm robots. Low-cost under-canopy robots can drive between crop rows under the plant canopy and accomplish tasks that are infeasible for over-the-canopy drones or larger agricultural equipment. However, autonomously navigating them under the canopy presents a number of challenges: unreliable GPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to leaves and weeds, and large variability in appearance over the season and across crop types. We address these challenges by building a modular system that leverages machine learning for robust and generalizable perception from monocular RGB images from low-cost cameras, and model predictive control for accurate control in challenging terrain. Our system, CropFollow, is able to autonomously drive 485 meters per intervention on average, outperforming a state-of-the-art LiDAR based system (286 meters per intervention) in extensive field testing spanning over 25 km.', \"title: How To Train Your HERON\\nabstract: In this paper we apply Deep Reinforcement Learning (Deep RL) and Domain Randomization to solve a navigation task in a natural environment relying solely on a 2D laser scanner. We train a model-based RL agent in simulation to follow lake and river shores and apply it on a real Unmanned Surface Vehicle in a zero-shot setup. We demonstrate that even though the agent has not been trained in the real world, it can fulfill its task successfully and adapt to changes in the robot's environment and dynamics. Finally, we show that the RL agent is more robust, faster, and more accurate than a state-aware Model-Predictive-Controller.\"]\n",
      "Title: Robot Laser Goose\n",
      "\n",
      "Abstract: The challenge of effectively deterring geese in various environments has been a persistent issue. This paper introduces a novel solution: the Robot Laser Goose, a system that leverages deep learning for target recognition and operates within a 500m range. The system utilizes a combination of RGB-D camera and radar sensor modalities, similar to previous studies on object interception. However, our focus is on the identification and deterrence of geese, achieving a remarkable 95% success rate. The Robot Laser Goose employs artificial neural networks for both geese detection and deterrence strategy prediction. The system's robustness and adaptability are demonstrated through its ability to operate in diverse environments and conditions. Furthermore, the Robot Laser Goose outperforms traditional methods in terms of range, accuracy, and success rate. This research contributes to the broader field of robotics and wildlife management, providing a scalable and efficient solution for geese deterrence. The findings also open up new avenues for the application of deep learning in wildlife interaction and management.\n"
     ]
    }
   ],
   "source": [
    "# 4. Retriveal of augmented generation\n",
    "\n",
    "message = \"Robot Laser Goose\"\n",
    "context = \"95% success rate in deterring Geese. Deep learning target recognition. 500m range\"\n",
    "\n",
    "abstracts = retrieve_info(message)\n",
    "print(\"Top matches: \")\n",
    "print(abstracts)\n",
    "response = chain.run(title=message, context=context,abstracts=abstracts)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
